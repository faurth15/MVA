{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Thomas_Fauré.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLlA9X9MF0Ma",
        "colab_type": "code",
        "outputId": "80aed5bb-b062-464d-b511-5386dd3491e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://www.di.ens.fr/willow/teaching/recvis18/assignment3/bird_dataset.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-26 15:48:40--  https://www.di.ens.fr/willow/teaching/recvis18/assignment3/bird_dataset.zip\n",
            "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
            "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘bird_dataset.zip’\n",
            "\n",
            "bird_dataset.zip        [                <=> ] 183.48M  9.47MB/s    in 21s     \n",
            "\n",
            "2019-11-26 15:49:02 (8.76 MB/s) - ‘bird_dataset.zip’ saved [192388716]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8X2FgQ1F2W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"bird_dataset.zip\",'r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBUi4VQ9F2ZV",
        "colab_type": "code",
        "outputId": "4c15989f-0630-45e3-dc59-79ffb858a7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install torch torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "1.3.1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUVROeNHF2bw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.autograd import Variable\n",
        "import PIL.Image as Image\n",
        "import os \n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHECbVrFF2eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# once the images are loaded, how do we pre-process them before being passed into the network\n",
        "# by default, we resize the images to 64 x 64 in size\n",
        "# and normalize them to mean = 0 and standard-deviation = 1 based on statistics collected from\n",
        "# the training set\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "      transforms.Resize((256)),\n",
        "      transforms.RandomRotation(45),\n",
        "      transforms.RandomResizedCrop(224),\n",
        "      transforms.RandomRotation(30),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.RandomVerticalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "      transforms.Resize((256)),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHPLvekH3_I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#variante model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "nclasses = 20 \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class Net(nn.Module):\n",
        "    def __init__(self,num_classes=20):\n",
        "        super(Net,self).__init__()\n",
        "        \n",
        "        self.res = models.resnet152(pretrained=True)\n",
        "        self.inc = models.inception_v3(pretrained=True)\n",
        "        \n",
        "\n",
        "\n",
        "     \n",
        "         \n",
        "        self.inc.aux_logits = False\n",
        "        num_features = self.inc.fc.in_features\n",
        "        self.inc.fc = nn.Linear(num_features,512)\n",
        "\n",
        "        \n",
        "        \n",
        "        num_features2 = self.res.fc.in_features\n",
        "        self.res.fc = nn.Linear(num_features2, 512)\n",
        "\n",
        "        \n",
        "\n",
        "        self.fc1 = nn.Linear(512*2,100)\n",
        "        self.fc2 = nn.Linear(100,20)\n",
        "\n",
        "\n",
        "      \n",
        "    def forward(self, input):\n",
        "        x1 = self.res(input)\n",
        "        x2 = self.inc(input)\n",
        "        x = torch.cat((x1,x2),1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        return self.fc2(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuqPeVcpWyO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "mod = models.resnet152(pretrained = True)\n",
        "\n",
        "mod.fc = nn.Linear(2048, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICNlazO3F2jS",
        "colab_type": "code",
        "outputId": "ed320355-0ef5-45c9-be44-37e10c07134d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#import argparse\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Training settings\n",
        "#parser = argparse.ArgumentParser(description='RecVis A3 training script')\n",
        "#parser.add_argument('--data', type=str, default='bird_dataset', metavar='D',\n",
        "#                   help=\"folder where data is located. train_images/ and val_images/ need to be found in the folder\")\n",
        "#parser.add_argument('--batch-size', type=int, default=64, metavar='B',\n",
        " #                   help='input batch size for training (default: 64)')\n",
        "#parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
        "  #                  help='number of epochs to train (default: 10)')\n",
        "#parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
        "   #                 help='learning rate (default: 0.01)')\n",
        "#parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
        "    #                help='SGD momentum (default: 0.5)')\n",
        "#parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "     #               help='random seed (default: 1)')\n",
        "#parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "      #              help='how many batches to wait before logging training status')\n",
        "#parser.add_argument('--experiment', type=str, default='experiment', metavar='E',\n",
        "       #             help='folder where experiment outputs are located.')\n",
        "#args = parser.parse_args()\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Create experiment folder\n",
        "if not os.path.isdir('nouveau'):\n",
        "    os.makedirs('nouveau')\n",
        "\n",
        "# Data initialization and loading\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder('bird_dataset' + '/train_images',\n",
        "                         transform=data_transforms),\n",
        "    batch_size=16, shuffle=True, num_workers=1)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder('bird_dataset' + '/val_images',\n",
        "                         transform=val_transforms),\n",
        "    batch_size=16, shuffle=False, num_workers=1)\n",
        "\n",
        "# Neural network and optimizer\n",
        "# We define neural net in model.py so that it can be reused by the evaluate.py script\n",
        "\n",
        "model = mod\n",
        "if use_cuda:\n",
        "    print('Using GPU')\n",
        "    model.cuda()\n",
        "else:\n",
        "    print('Using CPU')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "\n",
        "def validation():\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in val_loader:\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "        validation_loss += criterion(output, target).data.item()\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    validation_loss /= len(val_loader.dataset)\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        validation_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))\n",
        "\n",
        "\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size = 20, gamma=0.1)\n",
        "for epoch in range(1, 70 + 1):\n",
        "    train(epoch)\n",
        "    validation()\n",
        "    scheduler.step()\n",
        "    model_file = 'nouveau' + '/model_' + str(epoch) + '.pth'\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "    print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n",
            "Train Epoch: 1 [0/1082 (0%)]\tLoss: 0.891230\n",
            "Train Epoch: 1 [160/1082 (15%)]\tLoss: 0.601905\n",
            "Train Epoch: 1 [320/1082 (29%)]\tLoss: 0.194916\n",
            "Train Epoch: 1 [480/1082 (44%)]\tLoss: 0.297448\n",
            "Train Epoch: 1 [640/1082 (59%)]\tLoss: 0.463678\n",
            "Train Epoch: 1 [800/1082 (74%)]\tLoss: 0.371882\n",
            "Train Epoch: 1 [960/1082 (88%)]\tLoss: 0.266787\n",
            "\n",
            "Validation set: Average loss: 0.0234, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_1.pth. You can run `python evaluate.py --model nouveau/model_1.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 2 [0/1082 (0%)]\tLoss: 0.522903\n",
            "Train Epoch: 2 [160/1082 (15%)]\tLoss: 0.749278\n",
            "Train Epoch: 2 [320/1082 (29%)]\tLoss: 0.198878\n",
            "Train Epoch: 2 [480/1082 (44%)]\tLoss: 0.881888\n",
            "Train Epoch: 2 [640/1082 (59%)]\tLoss: 0.308589\n",
            "Train Epoch: 2 [800/1082 (74%)]\tLoss: 0.110813\n",
            "Train Epoch: 2 [960/1082 (88%)]\tLoss: 0.696061\n",
            "\n",
            "Validation set: Average loss: 0.0172, Accuracy: 97/103 (94%)\n",
            "Saved model to nouveau/model_2.pth. You can run `python evaluate.py --model nouveau/model_2.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 3 [0/1082 (0%)]\tLoss: 0.544562\n",
            "Train Epoch: 3 [160/1082 (15%)]\tLoss: 0.481776\n",
            "Train Epoch: 3 [320/1082 (29%)]\tLoss: 0.463260\n",
            "Train Epoch: 3 [480/1082 (44%)]\tLoss: 0.118771\n",
            "Train Epoch: 3 [640/1082 (59%)]\tLoss: 0.840527\n",
            "Train Epoch: 3 [800/1082 (74%)]\tLoss: 0.671812\n",
            "Train Epoch: 3 [960/1082 (88%)]\tLoss: 0.364660\n",
            "\n",
            "Validation set: Average loss: 0.0179, Accuracy: 93/103 (90%)\n",
            "Saved model to nouveau/model_3.pth. You can run `python evaluate.py --model nouveau/model_3.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 4 [0/1082 (0%)]\tLoss: 0.219990\n",
            "Train Epoch: 4 [160/1082 (15%)]\tLoss: 0.328269\n",
            "Train Epoch: 4 [320/1082 (29%)]\tLoss: 0.105393\n",
            "Train Epoch: 4 [480/1082 (44%)]\tLoss: 0.797365\n",
            "Train Epoch: 4 [640/1082 (59%)]\tLoss: 0.349507\n",
            "Train Epoch: 4 [800/1082 (74%)]\tLoss: 0.483850\n",
            "Train Epoch: 4 [960/1082 (88%)]\tLoss: 0.153406\n",
            "\n",
            "Validation set: Average loss: 0.0248, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_4.pth. You can run `python evaluate.py --model nouveau/model_4.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 5 [0/1082 (0%)]\tLoss: 0.345890\n",
            "Train Epoch: 5 [160/1082 (15%)]\tLoss: 0.328681\n",
            "Train Epoch: 5 [320/1082 (29%)]\tLoss: 1.090755\n",
            "Train Epoch: 5 [480/1082 (44%)]\tLoss: 0.361543\n",
            "Train Epoch: 5 [640/1082 (59%)]\tLoss: 0.171673\n",
            "Train Epoch: 5 [800/1082 (74%)]\tLoss: 0.662625\n",
            "Train Epoch: 5 [960/1082 (88%)]\tLoss: 0.220311\n",
            "\n",
            "Validation set: Average loss: 0.0307, Accuracy: 90/103 (87%)\n",
            "Saved model to nouveau/model_5.pth. You can run `python evaluate.py --model nouveau/model_5.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 6 [0/1082 (0%)]\tLoss: 0.271920\n",
            "Train Epoch: 6 [160/1082 (15%)]\tLoss: 0.527508\n",
            "Train Epoch: 6 [320/1082 (29%)]\tLoss: 1.068146\n",
            "Train Epoch: 6 [480/1082 (44%)]\tLoss: 0.599938\n",
            "Train Epoch: 6 [640/1082 (59%)]\tLoss: 0.296625\n",
            "Train Epoch: 6 [800/1082 (74%)]\tLoss: 0.056641\n",
            "Train Epoch: 6 [960/1082 (88%)]\tLoss: 0.199439\n",
            "\n",
            "Validation set: Average loss: 0.0228, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_6.pth. You can run `python evaluate.py --model nouveau/model_6.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 7 [0/1082 (0%)]\tLoss: 0.084574\n",
            "Train Epoch: 7 [160/1082 (15%)]\tLoss: 0.146696\n",
            "Train Epoch: 7 [320/1082 (29%)]\tLoss: 0.178263\n",
            "Train Epoch: 7 [480/1082 (44%)]\tLoss: 0.554802\n",
            "Train Epoch: 7 [640/1082 (59%)]\tLoss: 0.132483\n",
            "Train Epoch: 7 [800/1082 (74%)]\tLoss: 0.501495\n",
            "Train Epoch: 7 [960/1082 (88%)]\tLoss: 0.534603\n",
            "\n",
            "Validation set: Average loss: 0.0241, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_7.pth. You can run `python evaluate.py --model nouveau/model_7.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 8 [0/1082 (0%)]\tLoss: 0.193083\n",
            "Train Epoch: 8 [160/1082 (15%)]\tLoss: 0.432440\n",
            "Train Epoch: 8 [320/1082 (29%)]\tLoss: 0.086973\n",
            "Train Epoch: 8 [480/1082 (44%)]\tLoss: 0.472574\n",
            "Train Epoch: 8 [640/1082 (59%)]\tLoss: 0.200515\n",
            "Train Epoch: 8 [800/1082 (74%)]\tLoss: 0.048147\n",
            "Train Epoch: 8 [960/1082 (88%)]\tLoss: 0.118243\n",
            "\n",
            "Validation set: Average loss: 0.0337, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_8.pth. You can run `python evaluate.py --model nouveau/model_8.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 9 [0/1082 (0%)]\tLoss: 0.235463\n",
            "Train Epoch: 9 [160/1082 (15%)]\tLoss: 0.504421\n",
            "Train Epoch: 9 [320/1082 (29%)]\tLoss: 0.173575\n",
            "Train Epoch: 9 [480/1082 (44%)]\tLoss: 0.442862\n",
            "Train Epoch: 9 [640/1082 (59%)]\tLoss: 0.336361\n",
            "Train Epoch: 9 [800/1082 (74%)]\tLoss: 0.246480\n",
            "Train Epoch: 9 [960/1082 (88%)]\tLoss: 0.327710\n",
            "\n",
            "Validation set: Average loss: 0.0239, Accuracy: 93/103 (90%)\n",
            "Saved model to nouveau/model_9.pth. You can run `python evaluate.py --model nouveau/model_9.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 10 [0/1082 (0%)]\tLoss: 0.329699\n",
            "Train Epoch: 10 [160/1082 (15%)]\tLoss: 0.212092\n",
            "Train Epoch: 10 [320/1082 (29%)]\tLoss: 0.261162\n",
            "Train Epoch: 10 [480/1082 (44%)]\tLoss: 0.061461\n",
            "Train Epoch: 10 [640/1082 (59%)]\tLoss: 0.925448\n",
            "Train Epoch: 10 [800/1082 (74%)]\tLoss: 0.435637\n",
            "Train Epoch: 10 [960/1082 (88%)]\tLoss: 0.234622\n",
            "\n",
            "Validation set: Average loss: 0.0278, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_10.pth. You can run `python evaluate.py --model nouveau/model_10.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 11 [0/1082 (0%)]\tLoss: 0.317545\n",
            "Train Epoch: 11 [160/1082 (15%)]\tLoss: 0.150804\n",
            "Train Epoch: 11 [320/1082 (29%)]\tLoss: 0.159785\n",
            "Train Epoch: 11 [480/1082 (44%)]\tLoss: 0.909526\n",
            "Train Epoch: 11 [640/1082 (59%)]\tLoss: 0.134038\n",
            "Train Epoch: 11 [800/1082 (74%)]\tLoss: 0.670382\n",
            "Train Epoch: 11 [960/1082 (88%)]\tLoss: 0.064142\n",
            "\n",
            "Validation set: Average loss: 0.0268, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_11.pth. You can run `python evaluate.py --model nouveau/model_11.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 12 [0/1082 (0%)]\tLoss: 0.349069\n",
            "Train Epoch: 12 [160/1082 (15%)]\tLoss: 0.532251\n",
            "Train Epoch: 12 [320/1082 (29%)]\tLoss: 0.117884\n",
            "Train Epoch: 12 [480/1082 (44%)]\tLoss: 0.138931\n",
            "Train Epoch: 12 [640/1082 (59%)]\tLoss: 0.153237\n",
            "Train Epoch: 12 [800/1082 (74%)]\tLoss: 0.567006\n",
            "Train Epoch: 12 [960/1082 (88%)]\tLoss: 0.286941\n",
            "\n",
            "Validation set: Average loss: 0.0243, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_12.pth. You can run `python evaluate.py --model nouveau/model_12.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 13 [0/1082 (0%)]\tLoss: 0.031701\n",
            "Train Epoch: 13 [160/1082 (15%)]\tLoss: 0.147684\n",
            "Train Epoch: 13 [320/1082 (29%)]\tLoss: 0.159370\n",
            "Train Epoch: 13 [480/1082 (44%)]\tLoss: 0.530308\n",
            "Train Epoch: 13 [640/1082 (59%)]\tLoss: 0.639211\n",
            "Train Epoch: 13 [800/1082 (74%)]\tLoss: 0.394914\n",
            "Train Epoch: 13 [960/1082 (88%)]\tLoss: 0.696876\n",
            "\n",
            "Validation set: Average loss: 0.0250, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_13.pth. You can run `python evaluate.py --model nouveau/model_13.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 14 [0/1082 (0%)]\tLoss: 0.211454\n",
            "Train Epoch: 14 [160/1082 (15%)]\tLoss: 0.480307\n",
            "Train Epoch: 14 [320/1082 (29%)]\tLoss: 0.260895\n",
            "Train Epoch: 14 [480/1082 (44%)]\tLoss: 0.352647\n",
            "Train Epoch: 14 [640/1082 (59%)]\tLoss: 0.116270\n",
            "Train Epoch: 14 [800/1082 (74%)]\tLoss: 0.398665\n",
            "Train Epoch: 14 [960/1082 (88%)]\tLoss: 0.358526\n",
            "\n",
            "Validation set: Average loss: 0.0323, Accuracy: 87/103 (84%)\n",
            "Saved model to nouveau/model_14.pth. You can run `python evaluate.py --model nouveau/model_14.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 15 [0/1082 (0%)]\tLoss: 0.325464\n",
            "Train Epoch: 15 [160/1082 (15%)]\tLoss: 0.482147\n",
            "Train Epoch: 15 [320/1082 (29%)]\tLoss: 0.233699\n",
            "Train Epoch: 15 [480/1082 (44%)]\tLoss: 0.362551\n",
            "Train Epoch: 15 [640/1082 (59%)]\tLoss: 0.286598\n",
            "Train Epoch: 15 [800/1082 (74%)]\tLoss: 0.183566\n",
            "Train Epoch: 15 [960/1082 (88%)]\tLoss: 0.271747\n",
            "\n",
            "Validation set: Average loss: 0.0243, Accuracy: 87/103 (84%)\n",
            "Saved model to nouveau/model_15.pth. You can run `python evaluate.py --model nouveau/model_15.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 16 [0/1082 (0%)]\tLoss: 0.343359\n",
            "Train Epoch: 16 [160/1082 (15%)]\tLoss: 0.055851\n",
            "Train Epoch: 16 [320/1082 (29%)]\tLoss: 0.271599\n",
            "Train Epoch: 16 [480/1082 (44%)]\tLoss: 0.388628\n",
            "Train Epoch: 16 [640/1082 (59%)]\tLoss: 0.461928\n",
            "Train Epoch: 16 [800/1082 (74%)]\tLoss: 0.524570\n",
            "Train Epoch: 16 [960/1082 (88%)]\tLoss: 0.449126\n",
            "\n",
            "Validation set: Average loss: 0.0321, Accuracy: 88/103 (85%)\n",
            "Saved model to nouveau/model_16.pth. You can run `python evaluate.py --model nouveau/model_16.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 17 [0/1082 (0%)]\tLoss: 0.403258\n",
            "Train Epoch: 17 [160/1082 (15%)]\tLoss: 0.352039\n",
            "Train Epoch: 17 [320/1082 (29%)]\tLoss: 0.232828\n",
            "Train Epoch: 17 [480/1082 (44%)]\tLoss: 0.214391\n",
            "Train Epoch: 17 [640/1082 (59%)]\tLoss: 0.403378\n",
            "Train Epoch: 17 [800/1082 (74%)]\tLoss: 0.029771\n",
            "Train Epoch: 17 [960/1082 (88%)]\tLoss: 0.261777\n",
            "\n",
            "Validation set: Average loss: 0.0323, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_17.pth. You can run `python evaluate.py --model nouveau/model_17.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 18 [0/1082 (0%)]\tLoss: 0.022037\n",
            "Train Epoch: 18 [160/1082 (15%)]\tLoss: 0.196710\n",
            "Train Epoch: 18 [320/1082 (29%)]\tLoss: 0.426399\n",
            "Train Epoch: 18 [480/1082 (44%)]\tLoss: 0.128155\n",
            "Train Epoch: 18 [640/1082 (59%)]\tLoss: 0.135114\n",
            "Train Epoch: 18 [800/1082 (74%)]\tLoss: 0.325099\n",
            "Train Epoch: 18 [960/1082 (88%)]\tLoss: 1.067469\n",
            "\n",
            "Validation set: Average loss: 0.0307, Accuracy: 88/103 (85%)\n",
            "Saved model to nouveau/model_18.pth. You can run `python evaluate.py --model nouveau/model_18.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 19 [0/1082 (0%)]\tLoss: 0.221724\n",
            "Train Epoch: 19 [160/1082 (15%)]\tLoss: 0.268192\n",
            "Train Epoch: 19 [320/1082 (29%)]\tLoss: 0.427218\n",
            "Train Epoch: 19 [480/1082 (44%)]\tLoss: 0.517819\n",
            "Train Epoch: 19 [640/1082 (59%)]\tLoss: 0.211522\n",
            "Train Epoch: 19 [800/1082 (74%)]\tLoss: 0.667049\n",
            "Train Epoch: 19 [960/1082 (88%)]\tLoss: 0.078934\n",
            "\n",
            "Validation set: Average loss: 0.0303, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_19.pth. You can run `python evaluate.py --model nouveau/model_19.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 20 [0/1082 (0%)]\tLoss: 0.055804\n",
            "Train Epoch: 20 [160/1082 (15%)]\tLoss: 0.073732\n",
            "Train Epoch: 20 [320/1082 (29%)]\tLoss: 0.278795\n",
            "Train Epoch: 20 [480/1082 (44%)]\tLoss: 0.188707\n",
            "Train Epoch: 20 [640/1082 (59%)]\tLoss: 0.356972\n",
            "Train Epoch: 20 [800/1082 (74%)]\tLoss: 0.210239\n",
            "Train Epoch: 20 [960/1082 (88%)]\tLoss: 0.049116\n",
            "\n",
            "Validation set: Average loss: 0.0233, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_20.pth. You can run `python evaluate.py --model nouveau/model_20.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 21 [0/1082 (0%)]\tLoss: 0.421136\n",
            "Train Epoch: 21 [160/1082 (15%)]\tLoss: 0.317085\n",
            "Train Epoch: 21 [320/1082 (29%)]\tLoss: 0.110210\n",
            "Train Epoch: 21 [480/1082 (44%)]\tLoss: 0.156652\n",
            "Train Epoch: 21 [640/1082 (59%)]\tLoss: 0.757947\n",
            "Train Epoch: 21 [800/1082 (74%)]\tLoss: 0.318093\n",
            "Train Epoch: 21 [960/1082 (88%)]\tLoss: 0.159786\n",
            "\n",
            "Validation set: Average loss: 0.0235, Accuracy: 94/103 (91%)\n",
            "Saved model to nouveau/model_21.pth. You can run `python evaluate.py --model nouveau/model_21.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 22 [0/1082 (0%)]\tLoss: 0.421216\n",
            "Train Epoch: 22 [160/1082 (15%)]\tLoss: 0.289729\n",
            "Train Epoch: 22 [320/1082 (29%)]\tLoss: 0.086717\n",
            "Train Epoch: 22 [480/1082 (44%)]\tLoss: 0.082272\n",
            "Train Epoch: 22 [640/1082 (59%)]\tLoss: 0.075211\n",
            "Train Epoch: 22 [800/1082 (74%)]\tLoss: 0.150137\n",
            "Train Epoch: 22 [960/1082 (88%)]\tLoss: 0.421009\n",
            "\n",
            "Validation set: Average loss: 0.0249, Accuracy: 93/103 (90%)\n",
            "Saved model to nouveau/model_22.pth. You can run `python evaluate.py --model nouveau/model_22.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 23 [0/1082 (0%)]\tLoss: 0.111441\n",
            "Train Epoch: 23 [160/1082 (15%)]\tLoss: 0.626454\n",
            "Train Epoch: 23 [320/1082 (29%)]\tLoss: 0.623765\n",
            "Train Epoch: 23 [480/1082 (44%)]\tLoss: 0.676527\n",
            "Train Epoch: 23 [640/1082 (59%)]\tLoss: 0.063527\n",
            "Train Epoch: 23 [800/1082 (74%)]\tLoss: 0.086491\n",
            "Train Epoch: 23 [960/1082 (88%)]\tLoss: 0.361193\n",
            "\n",
            "Validation set: Average loss: 0.0235, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_23.pth. You can run `python evaluate.py --model nouveau/model_23.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 24 [0/1082 (0%)]\tLoss: 0.607909\n",
            "Train Epoch: 24 [160/1082 (15%)]\tLoss: 0.054626\n",
            "Train Epoch: 24 [320/1082 (29%)]\tLoss: 0.223313\n",
            "Train Epoch: 24 [480/1082 (44%)]\tLoss: 0.385450\n",
            "Train Epoch: 24 [640/1082 (59%)]\tLoss: 0.059943\n",
            "Train Epoch: 24 [800/1082 (74%)]\tLoss: 0.183836\n",
            "Train Epoch: 24 [960/1082 (88%)]\tLoss: 0.318031\n",
            "\n",
            "Validation set: Average loss: 0.0269, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_24.pth. You can run `python evaluate.py --model nouveau/model_24.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 25 [0/1082 (0%)]\tLoss: 0.299983\n",
            "Train Epoch: 25 [160/1082 (15%)]\tLoss: 0.480405\n",
            "Train Epoch: 25 [320/1082 (29%)]\tLoss: 0.167570\n",
            "Train Epoch: 25 [480/1082 (44%)]\tLoss: 0.593349\n",
            "Train Epoch: 25 [640/1082 (59%)]\tLoss: 0.250606\n",
            "Train Epoch: 25 [800/1082 (74%)]\tLoss: 0.048125\n",
            "Train Epoch: 25 [960/1082 (88%)]\tLoss: 0.454272\n",
            "\n",
            "Validation set: Average loss: 0.0231, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_25.pth. You can run `python evaluate.py --model nouveau/model_25.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 26 [0/1082 (0%)]\tLoss: 0.173853\n",
            "Train Epoch: 26 [160/1082 (15%)]\tLoss: 0.069027\n",
            "Train Epoch: 26 [320/1082 (29%)]\tLoss: 0.079097\n",
            "Train Epoch: 26 [480/1082 (44%)]\tLoss: 0.333999\n",
            "Train Epoch: 26 [640/1082 (59%)]\tLoss: 0.127873\n",
            "Train Epoch: 26 [800/1082 (74%)]\tLoss: 0.395227\n",
            "Train Epoch: 26 [960/1082 (88%)]\tLoss: 0.088330\n",
            "\n",
            "Validation set: Average loss: 0.0219, Accuracy: 93/103 (90%)\n",
            "Saved model to nouveau/model_26.pth. You can run `python evaluate.py --model nouveau/model_26.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 27 [0/1082 (0%)]\tLoss: 0.467927\n",
            "Train Epoch: 27 [160/1082 (15%)]\tLoss: 0.432869\n",
            "Train Epoch: 27 [320/1082 (29%)]\tLoss: 0.191457\n",
            "Train Epoch: 27 [480/1082 (44%)]\tLoss: 0.277989\n",
            "Train Epoch: 27 [640/1082 (59%)]\tLoss: 0.163754\n",
            "Train Epoch: 27 [800/1082 (74%)]\tLoss: 0.213933\n",
            "Train Epoch: 27 [960/1082 (88%)]\tLoss: 0.195775\n",
            "\n",
            "Validation set: Average loss: 0.0274, Accuracy: 88/103 (85%)\n",
            "Saved model to nouveau/model_27.pth. You can run `python evaluate.py --model nouveau/model_27.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 28 [0/1082 (0%)]\tLoss: 0.581409\n",
            "Train Epoch: 28 [160/1082 (15%)]\tLoss: 0.115242\n",
            "Train Epoch: 28 [320/1082 (29%)]\tLoss: 0.098895\n",
            "Train Epoch: 28 [480/1082 (44%)]\tLoss: 0.095082\n",
            "Train Epoch: 28 [640/1082 (59%)]\tLoss: 0.340767\n",
            "Train Epoch: 28 [800/1082 (74%)]\tLoss: 0.383290\n",
            "Train Epoch: 28 [960/1082 (88%)]\tLoss: 0.098633\n",
            "\n",
            "Validation set: Average loss: 0.0227, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_28.pth. You can run `python evaluate.py --model nouveau/model_28.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 29 [0/1082 (0%)]\tLoss: 0.376452\n",
            "Train Epoch: 29 [160/1082 (15%)]\tLoss: 0.506063\n",
            "Train Epoch: 29 [320/1082 (29%)]\tLoss: 0.299609\n",
            "Train Epoch: 29 [480/1082 (44%)]\tLoss: 0.037094\n",
            "Train Epoch: 29 [640/1082 (59%)]\tLoss: 0.035178\n",
            "Train Epoch: 29 [800/1082 (74%)]\tLoss: 0.007668\n",
            "Train Epoch: 29 [960/1082 (88%)]\tLoss: 0.226726\n",
            "\n",
            "Validation set: Average loss: 0.0254, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_29.pth. You can run `python evaluate.py --model nouveau/model_29.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 30 [0/1082 (0%)]\tLoss: 0.287925\n",
            "Train Epoch: 30 [160/1082 (15%)]\tLoss: 0.057327\n",
            "Train Epoch: 30 [320/1082 (29%)]\tLoss: 0.253507\n",
            "Train Epoch: 30 [480/1082 (44%)]\tLoss: 0.169335\n",
            "Train Epoch: 30 [640/1082 (59%)]\tLoss: 0.298514\n",
            "Train Epoch: 30 [800/1082 (74%)]\tLoss: 0.433950\n",
            "Train Epoch: 30 [960/1082 (88%)]\tLoss: 0.234227\n",
            "\n",
            "Validation set: Average loss: 0.0282, Accuracy: 90/103 (87%)\n",
            "Saved model to nouveau/model_30.pth. You can run `python evaluate.py --model nouveau/model_30.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 31 [0/1082 (0%)]\tLoss: 0.094737\n",
            "Train Epoch: 31 [160/1082 (15%)]\tLoss: 0.418063\n",
            "Train Epoch: 31 [320/1082 (29%)]\tLoss: 0.423304\n",
            "Train Epoch: 31 [480/1082 (44%)]\tLoss: 0.602844\n",
            "Train Epoch: 31 [640/1082 (59%)]\tLoss: 0.150285\n",
            "Train Epoch: 31 [800/1082 (74%)]\tLoss: 0.334520\n",
            "Train Epoch: 31 [960/1082 (88%)]\tLoss: 0.309432\n",
            "\n",
            "Validation set: Average loss: 0.0274, Accuracy: 89/103 (86%)\n",
            "Saved model to nouveau/model_31.pth. You can run `python evaluate.py --model nouveau/model_31.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 32 [0/1082 (0%)]\tLoss: 0.428488\n",
            "Train Epoch: 32 [160/1082 (15%)]\tLoss: 0.205444\n",
            "Train Epoch: 32 [320/1082 (29%)]\tLoss: 0.069004\n",
            "Train Epoch: 32 [480/1082 (44%)]\tLoss: 0.290751\n",
            "Train Epoch: 32 [640/1082 (59%)]\tLoss: 0.111209\n",
            "Train Epoch: 32 [800/1082 (74%)]\tLoss: 0.420369\n",
            "Train Epoch: 32 [960/1082 (88%)]\tLoss: 0.159970\n",
            "\n",
            "Validation set: Average loss: 0.0248, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_32.pth. You can run `python evaluate.py --model nouveau/model_32.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 33 [0/1082 (0%)]\tLoss: 0.007209\n",
            "Train Epoch: 33 [160/1082 (15%)]\tLoss: 0.302511\n",
            "Train Epoch: 33 [320/1082 (29%)]\tLoss: 0.167119\n",
            "Train Epoch: 33 [480/1082 (44%)]\tLoss: 0.101888\n",
            "Train Epoch: 33 [640/1082 (59%)]\tLoss: 0.063621\n",
            "Train Epoch: 33 [800/1082 (74%)]\tLoss: 0.248215\n",
            "Train Epoch: 33 [960/1082 (88%)]\tLoss: 0.201010\n",
            "\n",
            "Validation set: Average loss: 0.0219, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_33.pth. You can run `python evaluate.py --model nouveau/model_33.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 34 [0/1082 (0%)]\tLoss: 0.184384\n",
            "Train Epoch: 34 [160/1082 (15%)]\tLoss: 0.361573\n",
            "Train Epoch: 34 [320/1082 (29%)]\tLoss: 0.731609\n",
            "Train Epoch: 34 [480/1082 (44%)]\tLoss: 0.530204\n",
            "Train Epoch: 34 [640/1082 (59%)]\tLoss: 0.027439\n",
            "Train Epoch: 34 [800/1082 (74%)]\tLoss: 0.405584\n",
            "Train Epoch: 34 [960/1082 (88%)]\tLoss: 0.333844\n",
            "\n",
            "Validation set: Average loss: 0.0275, Accuracy: 90/103 (87%)\n",
            "Saved model to nouveau/model_34.pth. You can run `python evaluate.py --model nouveau/model_34.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 35 [0/1082 (0%)]\tLoss: 0.216392\n",
            "Train Epoch: 35 [160/1082 (15%)]\tLoss: 0.572211\n",
            "Train Epoch: 35 [320/1082 (29%)]\tLoss: 0.068149\n",
            "Train Epoch: 35 [480/1082 (44%)]\tLoss: 0.097545\n",
            "Train Epoch: 35 [640/1082 (59%)]\tLoss: 0.349568\n",
            "Train Epoch: 35 [800/1082 (74%)]\tLoss: 0.302538\n",
            "Train Epoch: 35 [960/1082 (88%)]\tLoss: 0.579649\n",
            "\n",
            "Validation set: Average loss: 0.0236, Accuracy: 90/103 (87%)\n",
            "Saved model to nouveau/model_35.pth. You can run `python evaluate.py --model nouveau/model_35.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 36 [0/1082 (0%)]\tLoss: 0.367947\n",
            "Train Epoch: 36 [160/1082 (15%)]\tLoss: 0.013169\n",
            "Train Epoch: 36 [320/1082 (29%)]\tLoss: 0.084270\n",
            "Train Epoch: 36 [480/1082 (44%)]\tLoss: 0.301715\n",
            "Train Epoch: 36 [640/1082 (59%)]\tLoss: 0.327295\n",
            "Train Epoch: 36 [800/1082 (74%)]\tLoss: 0.396673\n",
            "Train Epoch: 36 [960/1082 (88%)]\tLoss: 0.163638\n",
            "\n",
            "Validation set: Average loss: 0.0302, Accuracy: 90/103 (87%)\n",
            "Saved model to nouveau/model_36.pth. You can run `python evaluate.py --model nouveau/model_36.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 37 [0/1082 (0%)]\tLoss: 0.184981\n",
            "Train Epoch: 37 [160/1082 (15%)]\tLoss: 0.091358\n",
            "Train Epoch: 37 [320/1082 (29%)]\tLoss: 0.434665\n",
            "Train Epoch: 37 [480/1082 (44%)]\tLoss: 0.660871\n",
            "Train Epoch: 37 [640/1082 (59%)]\tLoss: 0.240891\n",
            "Train Epoch: 37 [800/1082 (74%)]\tLoss: 0.437359\n",
            "Train Epoch: 37 [960/1082 (88%)]\tLoss: 0.433028\n",
            "\n",
            "Validation set: Average loss: 0.0287, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_37.pth. You can run `python evaluate.py --model nouveau/model_37.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 38 [0/1082 (0%)]\tLoss: 0.233728\n",
            "Train Epoch: 38 [160/1082 (15%)]\tLoss: 0.677163\n",
            "Train Epoch: 38 [320/1082 (29%)]\tLoss: 0.484064\n",
            "Train Epoch: 38 [480/1082 (44%)]\tLoss: 0.184596\n",
            "Train Epoch: 38 [640/1082 (59%)]\tLoss: 0.137737\n",
            "Train Epoch: 38 [800/1082 (74%)]\tLoss: 0.007097\n",
            "Train Epoch: 38 [960/1082 (88%)]\tLoss: 0.348894\n",
            "\n",
            "Validation set: Average loss: 0.0330, Accuracy: 92/103 (89%)\n",
            "Saved model to nouveau/model_38.pth. You can run `python evaluate.py --model nouveau/model_38.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 39 [0/1082 (0%)]\tLoss: 0.026422\n",
            "Train Epoch: 39 [160/1082 (15%)]\tLoss: 0.071007\n",
            "Train Epoch: 39 [320/1082 (29%)]\tLoss: 0.176397\n",
            "Train Epoch: 39 [480/1082 (44%)]\tLoss: 0.056593\n",
            "Train Epoch: 39 [640/1082 (59%)]\tLoss: 0.015973\n",
            "Train Epoch: 39 [800/1082 (74%)]\tLoss: 0.102345\n",
            "Train Epoch: 39 [960/1082 (88%)]\tLoss: 0.237500\n",
            "\n",
            "Validation set: Average loss: 0.0247, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_39.pth. You can run `python evaluate.py --model nouveau/model_39.pth` to generate the Kaggle formatted csv file\n",
            "\n",
            "Train Epoch: 40 [0/1082 (0%)]\tLoss: 0.274797\n",
            "Train Epoch: 40 [160/1082 (15%)]\tLoss: 0.147868\n",
            "Train Epoch: 40 [320/1082 (29%)]\tLoss: 0.383563\n",
            "Train Epoch: 40 [480/1082 (44%)]\tLoss: 0.688800\n",
            "Train Epoch: 40 [640/1082 (59%)]\tLoss: 0.304228\n",
            "Train Epoch: 40 [800/1082 (74%)]\tLoss: 0.180287\n",
            "Train Epoch: 40 [960/1082 (88%)]\tLoss: 0.014994\n",
            "\n",
            "Validation set: Average loss: 0.0264, Accuracy: 91/103 (88%)\n",
            "Saved model to nouveau/model_40.pth. You can run `python evaluate.py --model nouveau/model_40.pth` to generate the Kaggle formatted csv file\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA0w8n6RF2mM",
        "colab_type": "code",
        "outputId": "04dba8fe-96c3-4eeb-dd75-f3025c213558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser(description='RecVis A3 evaluation script')\n",
        "#parser.add_argument('--data', type=str, default='bird_dataset', metavar='D',\n",
        "#                    help=\"folder where data is located. test_images/ need to be found in the folder\")\n",
        "#parser.add_argument('--model', type=str, metavar='M',\n",
        "#                    help=\"the model file to be evaluated. Usually it is of the form model_X.pth\")\n",
        "#parser.add_argument('--outfile', type=str, default='experiment/kaggle.csv', metavar='D',\n",
        " #                   help=\"name of the output csv file\")\n",
        "\n",
        "#args = parser.parse_args()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "state_dict = torch.load('/content/nouveau/model_12.pth')\n",
        "model = model\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "if use_cuda:\n",
        "    print('Using GPU')\n",
        "    model.cuda()\n",
        "else:\n",
        "    print('Using CPU')\n",
        "\n",
        "\n",
        "test_dir = 'bird_dataset' + '/test_images/mistery_category'\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "\n",
        "output_file = open('kaggle.csv', \"w\")\n",
        "output_file.write(\"Id,Category\\n\")\n",
        "for f in tqdm(os.listdir(test_dir)):\n",
        "    if 'jpg' in f:\n",
        "        data = data_transforms(pil_loader(test_dir + '/' + f))\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "        output = model(data)\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
        "\n",
        "output_file.close()\n",
        "\n",
        "print(\"Succesfully wrote \" + ', you can upload this file to the kaggle competition website')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 4/517 [00:00<00:15, 32.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 517/517 [00:15<00:00, 34.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Succesfully wrote , you can upload this file to the kaggle competition website\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43uds73aF2pA",
        "colab_type": "code",
        "outputId": "d83ed90c-9a99-4cea-e644-af6b92f3c45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataParallel(\n",
            "  (module): Net(\n",
            "    (res): ResNet(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (6): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (7): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (6): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (7): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (8): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (9): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (10): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (11): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (12): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (13): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (14): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (15): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (16): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (17): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (18): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (19): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (20): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (21): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (22): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (23): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (24): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (25): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (26): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (27): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (28): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (29): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (30): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (31): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (32): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (33): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (34): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (35): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "    (inc): Inception3(\n",
            "      (Conv2d_1a_3x3): BasicConv2d(\n",
            "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Conv2d_2a_3x3): BasicConv2d(\n",
            "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Conv2d_2b_3x3): BasicConv2d(\n",
            "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Conv2d_3b_1x1): BasicConv2d(\n",
            "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Conv2d_4a_3x3): BasicConv2d(\n",
            "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (Mixed_5b): InceptionA(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_2): BasicConv2d(\n",
            "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_5c): InceptionA(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_1): BasicConv2d(\n",
            "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_2): BasicConv2d(\n",
            "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_5d): InceptionA(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_1): BasicConv2d(\n",
            "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch5x5_2): BasicConv2d(\n",
            "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_6a): InceptionB(\n",
            "        (branch3x3): BasicConv2d(\n",
            "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_6b): InceptionC(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_3): BasicConv2d(\n",
            "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_4): BasicConv2d(\n",
            "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_5): BasicConv2d(\n",
            "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_6c): InceptionC(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_2): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_3): BasicConv2d(\n",
            "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_4): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_5): BasicConv2d(\n",
            "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_6d): InceptionC(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_2): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_3): BasicConv2d(\n",
            "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_4): BasicConv2d(\n",
            "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_5): BasicConv2d(\n",
            "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_6e): InceptionC(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7_3): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_3): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_4): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7dbl_5): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (AuxLogits): InceptionAux(\n",
            "        (conv0): BasicConv2d(\n",
            "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv1): BasicConv2d(\n",
            "          (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
            "      )\n",
            "      (Mixed_7a): InceptionD(\n",
            "        (branch3x3_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7x3_1): BasicConv2d(\n",
            "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7x3_2): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7x3_3): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch7x7x3_4): BasicConv2d(\n",
            "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_7b): InceptionE(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_1): BasicConv2d(\n",
            "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_2a): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_2b): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3a): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3b): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (Mixed_7c): InceptionE(\n",
            "        (branch1x1): BasicConv2d(\n",
            "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_1): BasicConv2d(\n",
            "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_2a): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3_2b): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_1): BasicConv2d(\n",
            "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_2): BasicConv2d(\n",
            "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3a): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch3x3dbl_3b): BasicConv2d(\n",
            "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
            "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (branch_pool): BasicConv2d(\n",
            "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "    (fc1): Linear(in_features=1024, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LtvggeVF2sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ha-oR7F2vg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbVDV8ghF2yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXd92YtEF22G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx5cEU5SF245",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}